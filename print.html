<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>teja&#x27;s notes</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
                <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="Preface.html">Preface</a></li><li class="chapter-item expanded "><a href="N_Linux_Networking.html">N. Linux_Networking</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="N/1_setup_qemu.html">N1. Setup Qemu</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="N/1_setup_uml.html">N1. Setup UML (older)</a></li></ol></li><li class="chapter-item expanded "><a href="N/2_Packet_RX_Basic.html">N2. Packet RX path 1 : Basic</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="N/2_1-3_top_half_processing.html">N2.1-N2.3 Top Half Processing</a></li><li class="chapter-item expanded "><a href="N/2_4-5_bottom_half_processing.html">N2.4-N2.5 Bottom Half Processing</a></li><li class="chapter-item expanded "><a href="N/2_6-7_ip_and_udp_processing.html">N2.6-N2.7 IP and UDP Processing</a></li></ol></li><li class="chapter-item expanded "><a href="N/3_Packet_TX_Basic.html">N3. Packet TX path 1 : Basic</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="N/3_1-3_sendmsg_from_userspace.html">N3.1-3.2 sendmsg() from userspace</a></li><li class="chapter-item expanded "><a href="N/3_3-4_alloc_and_send_skb.html">N3.3-3.4 alloc skb and send_skb</a></li><li class="chapter-item expanded "><a href="N/3_5-8_net_tx_and_driver_xmit.html">N3.5-3.8 NET_TX and driver xmit</a></li></ol></li><li class="chapter-item expanded "><a href="N/4_Socket_Programming_BTS.html">N4. (WIP) Socket Programming BTS</a></li></ol></li><li class="chapter-item expanded "><a href="M_Miscellaneous.html">M. Miscellaneous</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="M/1_CSE222a_Notes.html">CSE 222a, Notes</a></li><li class="chapter-item expanded "><a href="M/2_Cache_Side_Channel_Attacks.html">Cache Side Channel Attacks</a></li></ol></li><li class="chapter-item expanded "><a href="Appendix.html">A. Appendix</a></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">teja&#x27;s notes</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="0-preface"><a class="header" href="#0-preface">0. Preface</a></h1>
<ol>
<li>
<p>This is a simple collection of some of my notes. Some parts are incomplete, I'll
mark them with a ​&quot;WIP&quot;​ tag.</p>
</li>
<li>
<p>No license. I am doing this for myself. If you find it helpful, that is great.</p>
</li>
<li>
<p>Please send any feedback/errors to tteja2010 at gmail dot com .</p>
</li>
<li>
<p>I went through the Linux code and then later read/understood OS concepts. My way of looking at OS related topics is hence biased towards Linux, which may make my notes weird/unnatural.</p>
</li>
<li>
<p>I like my notes to be very simple. If I were to forget the past couple of years of my life due to an accident, I should still be able to catch up by  going through my notes (Assuming I discover that I had written these notes) That is how simple it should be. I don't want to assume anything about the reader (my amnesiac self who just completed his bachelors) while writing them.</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linux-networking-notes"><a class="header" href="#linux-networking-notes">Linux Networking Notes</a></h1>
<p>This subsection contains my notes on the networking subsystem.</p>
<p>Before we jump in,  notes on how packets are represented in Linux and visualizing their processing.</p>
<h3 id="n01-packet-representation-in-linux"><a class="header" href="#n01-packet-representation-in-linux">N.0.1 Packet Representation in Linux</a></h3>
<p>Packets are represented using sk_buff (socket buffer) in Linux. The struct is declared in <code>include/linux/skbuff.h</code>. I will call a packet as skb interchangeably from now on. The sk_buff struct contains two parts, the packet data and it's meta data.</p>
<p>Firstly it contains pointers to the actual data. The actual packet with Ethernet, IP, transport headers and payload that has made it's way over the network will be put in some memory that is allocated. The simplest way this is done is to allocate a contiguous memory block which will contain the whole packet. (We will see in later pages how very large packets can be created using lists of such blocks or how number of data copies can be reduced by having multiple small chunks of data.) skb-&gt;head points to the start of the this block, and skb-&gt;end points to the end of this block. The whole block need not contain meaningful data. skb-&gt;data points to the place where the packet data starts, and skb-&gt;tail points to the place where the packet data ends. This allows the packet to have some head room and tail room if the packet needs to expand. These four pointers are used to point to the actual data. They are placed at the end of the sk_buff struct. David Miller's page on <a href="http://vger.kernel.org/%7Edavem/skb_data.html">skb_data</a> describes skb data in greater detail.</p>
<p>An image from the above page: </p>
<p><img src="imgs/01_skb_layout.png" alt="skb_layout" /></p>
<p>Additionally the skb contains lots of meta data. Without checking the actual data, a fully filled skb can provide the protocol information, checksum details, it's corresponding socket, etc. The meta data is information that is extracted from the packet data or information attached to the current packet that can be used by all the layers. A few of these fields are explained in David Miller's page <a href="http://vger.kernel.org/%7Edavem/skb.html">How SKBs work</a>.</p>
<p>This is similar to how photographs are saved. One part is the actual image, the second part is meta data like it's dimensions, ISO, aperture, camera model, location information, etc. The meta data by itself is not useful but adds detail to the original data.</p>
<p>I'll add a page which describes the skb and it's fields in greater detail. TODO. </p>
<h3 id="n02-visualizing-packet-processing"><a class="header" href="#n02-visualizing-packet-processing">N.0.2 Visualizing Packet Processing</a></h3>
<p>This is not a standard way of visualizing, but I think this is the right way to visualize packet processing and cant visualize in any other way. Receiving packets is in the bottom to top direction. And transmitting packets is in the top to bottom direction. Forwarding to a different layer is left to right.
While receiving packets, drivers receive data first. The bottom most layer where the drivers stay. The drivers hand over the packet to the core network. The core networking code then passes it over to the right protocol stack(s). After the protocol stack processing is done, it enters socket layer, from where the user picks up the packet. </p>
<img src="imgs/visualize_pkt_proc.png" alt="visualize"  />
<h4 id="n021-top-half-and-bottom-half-processing"><a class="header" href="#n021-top-half-and-bottom-half-processing">N.0.2.1 Top half and bottom half processing</a></h4>
<p>The path from the driver to the socket queue is divided into two halves.</p>
<p>The top half happens first, the driver gets the raw packet and creates a skb. After an skb is created it calls functions to hand it over to the core networking code. The top half before exiting schedules the bottom half. Top half runs per packet and exits.</p>
<p>The bottom half begins picking up each packet and starts processing them. The packet is passed trough IP, UDP stacks and finally enqueues it into the socket queue. This is done for a bunch of packets. If there are packets that are still pending, the bottom half schedules itself and exits.</p>
<p>IMPORTANT:</p>
<ol>
<li>The top half is below the bottom half in my figure.</li>
<li>I can use bottom half OR softirq processing interchangeably.</li>
<li>Softirq processing done while receiving packets is also called NET_RX processing. I can use this as well. :)</li>
<li>Core network code runs in both these halves. But most of it is in softirq processing. </li>
</ol>
<p>With this, basic information we can start describing the RX and TX processing paths.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n1-setup-qemu"><a class="header" href="#n1-setup-qemu">N1 Setup Qemu</a></h1>
<p>These notes are based on the following videos:</p>
<ol>
<li><a href="https://www.youtube.com/watch?v=kQFYfIXhahs">Create an Arch Linux QEMU installation</a>: Installing arch is too involved, so I chose to install ubuntu.</li>
<li><a href="https://www.youtube.com/watch?v=unizGCcZg3Y">GDB on the Linux Kernel</a>. The script below is the same as the one described in this video.</li>
</ol>
<p>Like described in the article on UML Setup, I like to learn by running a VM and attaching it via GDB. KVM and QEMU are the newer well supported VM solutions. This page is a tutorial on how to launch a debug instance in QEMU and attach to it using GDB.</p>
<p>Install QEMU (check Qemu download page for distribution specific instructions). </p>
<h3 id="n11-setup--build-a-kernel-with-debug-symbols"><a class="header" href="#n11-setup--build-a-kernel-with-debug-symbols">N1.1 Setup &amp; build a kernel with debug symbols</a></h3>
<p>Clone the kernel from kernel git repo. </p>
<pre><code class="language-shell">git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git linux
cd linux 
</code></pre>
<p>Next run &quot;make menuconfig&quot; to modify a few configurations. On running the command a ncurses interface to enable/disable options will open. Use the arrow keys to move between options and 'Y' and 'N' keys to include/exclude options.
First enable CONFIG_DEBUG_KERNEL. It is located under &quot;Kernel hacking&quot; as &quot;Kernel debugging&quot;.
Next disable RANDOMIZE_MEMORY_PHYSICAL_PADDING. It is located under &quot;Processor type and features&quot; as &quot;Randomize the kernel memory sections&quot;.
After making the changes save the changes and exit the menuconfig interface. A lot of youtube videos explain the same in detail, check them if confused. To reset any changes delete the &quot;.config&quot; file and start over by running &quot;make defconfig&quot;. Finally run make.
It will take some to compile the kernel. Meanwhile move to the next step and install a VM in qemu. </p>
<pre><code>make defconfig
make menuconfig     # enable CONFIG_DEBUG_KERNEL,
                    # disable RANDOMIZE_MEMORY_PHYSICAL_PADDING

make -j1            # replace 1 with the number of CPUs that make
                    # should use to build the kernel.  
</code></pre>
<h3 id="n12-create-an-image-for-qemu"><a class="header" href="#n12-create-an-image-for-qemu">N1.2 Create an image for QEMU</a></h3>
<p>Move out of the linux directory and create an image for QEMU.</p>
<p><code>qemu-img create kernel-dev.img 20G</code> </p>
<p>Next download ubuntu's minimal iso image from here. It is a small 64MB file which can be used for a minimal Linux Installation. Move the downloaded mini.iso file into the same directory.
Finally save the script below as start.sh.</p>
<pre><code class="language-bash">#!/bin/bash

#startup.sh

KERNEL=&quot;linux/arch/x86_64/boot/bzImage&quot;
RAM=1G
CPUS=2
DISK=&quot;kernel-dev.img&quot;

if [ $# -eq 1 ]
then
	qemu-system-x86_64 \
		-enable-kvm \
		-smp $CPUS \
		-drive file=$DISK,format=raw,index=0,media=disk \
		-m $RAM \
		-serial stdio \
		-drive file=$1,index=2,media=cdrom  ## comment to run vanilla install
		# use this option to boot using a cd iso
else
	qemu-system-x86_64 \
		-enable-kvm \
		-smp $CPUS \
		-drive file=$DISK,format=raw,index=0,media=disk \
		-m $RAM \
		-serial stdio \
		-kernel $KERNEL \
		-S -s \
		-cpu host \
		-append &quot;root=/dev/sda1&quot; \
		-net user,hostfwd=tcp::5555-:22 -net nic \
fi
</code></pre>
<p>The above script when given an ISO file passes it as an CD to the QEMU instance. This way we can install ubuntu into &quot;kernel-dev.img&quot;. </p>
<p>If no arguments are provided it tries to run the OS installed on kernel-dev.img. This way we can use the script to start the VM after we have completed installing ubuntu. At this point the directory structure should look like this:</p>
<pre><code>  .
  ├── kernel-dev.img
  ├── linux
  ├── mini.iso
  └── startup.sh 
</code></pre>
<p>First to install ubuntu run: (if superuser privileges needed, run with sudo) </p>
<pre><code>./startup.sh mini.iso
</code></pre>
<p>Go through all the steps and install ubuntu. A lot of YouTube videos show the  complete process. Use them as a reference if necessary.</p>
<p>Once the installation is complete, comment out  the line which provides the cdrom option to boot into an vanilla ubuntu install.</p>
<pre><code>./startup.sh mini.iso  #cdrom line commented
</code></pre>
<p>Now wait for the kernel installation to complete. Then run the script without and arguments to boot into the kernel we built.</p>
<pre><code>./startup.sh
</code></pre>
<p>The boot will wait for GDB to connect. On a separate terminal run:</p>
<pre><code class="language-bash">gdb linux/vmlinux
</code></pre>
<p>Within the gdb prompt then run &quot;target remote :1234&quot; to connect to QEMU. The <code>bt</code> command should then show some stack within QEMU.
Run &quot;hbreak start_kernel&quot; to add a hardware breakpoint at start_kernel() and then run &quot;continue&quot;. The VM would then begin booting and will stop in the start_kernel function.</p>
<p>Add other hardware breakpoints (since QEMU uses hardware acceleration for Virtualization, normal SW breakpoints will not work) and start tinkering.
The network options are similar to those of UML. Thses options have been commented in the above script. </p>
<h3 id="n13-network-setup"><a class="header" href="#n13-network-setup">N1.3 Network setup</a></h3>
<p>By default the script provides an interface via which the VM can access both internet and the host machine (over ssh). This is the SLIRP networking mode. Follow the link <a href="https://wiki.qemu.org/Documentation/Networking#User_Networking_.28SLIRP.29">here</a> to read more.</p>
<p>The interface will be provided but the interface will not have an address. Run dhclient on the interface so it is assigned an address. Next install an sshserver, if not installed, so we can access the guest over ssh.</p>
<pre><code class="language-bash">sudo dhclient eth0  # provide the right interface name.
sudo apt update     # needed to update apt cache.
sudo apt install openssh-server
</code></pre>
<p>Finally to login into the guest machine, run:</p>
<pre><code>ssh -p 5555 localhost 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n1-setup-uml-older"><a class="header" href="#n1-setup-uml-older">N1. Setup UML (older)</a></h1>
<p>SKIP THIS IF YOU WERE ABLE TO SETUP QEMU. This page is here only for completeness, but is not necessary to experiment with a kernel.</p>
<hr />
<p>I prefer to learn/tinker with linux using UML. UML is User mode linux, which is a simple VM. It emulates a uniprocessor linux system, and can run even on machines with very old hardware (like my laptop). Check the UML homepage for additional details. This page is just to a simple tutorial on how to build and run it. I have also added sections on how to attach it to GDB for easy debugging and a section on how to setup basic networking between multiple UMLs which you can skip in the first read. </p>
<h3 id="n01-clone-and-build-the-kernel"><a class="header" href="#n01-clone-and-build-the-kernel">N0.1 Clone and build the kernel</a></h3>
<p>Clone the kernel, and build it.
Make defconfig sets up the default kernel config. The kernel config is a set of kernel features which will either be compiled into the kernel or will be compiled as modules. The architecture for which we are configuring is the UM (user mode) architechture. While building the config, you may be prompted to choose a configuration. Press enter to choose the default configuration.
Once the configuration is done, a &quot;.config&quot; file will be populated with the chosen options.
Begin compiling the code. Based on your machine's CPU capabities, replace 1 with the number of parallel compilation jobs you want make to run. Please remain patient as the very first compilation will take some time.</p>
<pre><code class="language-bash">git clone git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git linux
cd linux
make defconfig ARCH=um
make -j1 ARCH=um 
</code></pre>
<p>A new file &quot;linux&quot; will be created. This is the UML executable which runs as an application on the host machine. It needs a few other files and arguments to run, which will be explained in the next section.</p>
<h3 id="n011-kernel-config-optional"><a class="header" href="#n011-kernel-config-optional">N0.1.1 Kernel config [OPTIONAL]</a></h3>
<p>The kernel src includes a neat way to configure the kernel. Run make menuconfig to configure various options. A ncurses interface should start up, with the instructions provided in the top. Pressing Y includes a config and N excludes a config. After configuring the kernel, save and exit the config. The .config file will get updated with the necessary configuration.
To build the uml binary with debug symbols, edit KBUILD_HOSTCFLAGS in the Makefile. Just add a '-g' option at the end. The kernel can then be rebuilt with the new configuration.</p>
<pre><code>make menuconfig ARCH=um
make -j1 ARCH=um 
</code></pre>
<p>Makefile diff:</p>
<pre><code> HOSTCC       = gcc
 HOSTCXX      = g++
-KBUILD_HOSTCFLAGS   := -Wall -Wmissing-prototypes -Wstrict-prototypes -O2 \
+KBUILD_HOSTCFLAGS   := -Wall -Wmissing-prototypes -Wstrict-prototypes -O2 -g \
                -fomit-frame-pointer -std=gnu89 $(HOST_LFS_CFLAGS) \
                $(HOSTCFLAGS)
 KBUILD_HOSTCXXFLAGS := -O2 $(HOST_LFS_CFLAGS) $(HOSTCXXFLAGS) 
</code></pre>
<h3 id="n02-rootfs-setup"><a class="header" href="#n02-rootfs-setup">N0.2 Rootfs setup</a></h3>
<p>We have a compiled kernel but we still need a init script and other userspace  programs. We need a rootfs to emulate the disk. There are a lot of blogs  which describe how to build a rootfs. We will download a debian rootfs that is  provided by google. 
Run the following command to download the rootfs and uncompress it. On  uncompressing it a 1GiB net_test.rootfs.20150203 should be available.</p>
<pre><code>wget -nv https://dl.google.com/dl/android/net_test.rootfs.20150203.xz
unxz net_test.rootfs.20150203.xz 
</code></pre>
<p>Note: If the above download does not work, check the <a href="https://android.googlesource.com/kernel/tests/+/master/net/test/run_net_test.sh#83">Google nettest script</a> where the rootfs link can be found. Google uses    UML to run nettests on the kernels OEMs ship to check for possible bugs.<br />
We now have everything needed to run the UML.</p>
<p>.2.1 Adding files/programs to rootfs [OPTIONAL]</p>
<p>The rootfs now contains certain programs and a init script which can be used to boot into the UML. We may need to install programs for our testing or need to move files between UML and the host OS. By mounting it into a directory the rootfs contents can be accessed.</p>
<pre><code>mkdir temp
sudo mount net_test.rootfs.20150203 temp
</code></pre>
<p>Copying from/to the directory is equivalent to copying files from/to the UML. In the below example I am copying my tmux configuration files into the rootfs. When I boot into UML, I will find the config file in the home directory. (Super user privilages are needed to add/remove files from the rootfs). </p>
<pre><code>sudo cp ~/.tmux.conf temp/home/ 
</code></pre>
<p>By chroot-ing into the mounted directory, any necessary programs can be installed. On running chroot, you will able to edit the rootfs with super user privilages. I am installing tmux in the below code and then exiting the chroot shell.</p>
<pre><code>sudo chroot temp
apt-get update
apt-get install tmux
exit  
</code></pre>
<p>Once all the changes are done, unmount the rootfs.</p>
<pre><code>sudo umount temp
</code></pre>
<h4 id="0211-apt-get-is-failing"><a class="header" href="#0211-apt-get-is-failing">0.2.1.1 apt-get is failing</a></h4>
<p>Note: apt-get update may fail printing the following error:</p>
<pre><code>Err http://ftp.jp.debian.org wheezy Release.gpg
Connection failed
</code></pre>
<p>If the /etc/hosts file contains an address for a particular hostname, the  system will not do an additional DNS lookup. In this case, the address  corresponding to ftp.jp.debian.org is incorrect, causing apt-get to fail.  Run the command &quot;dig ftp.jp.debian.org&quot; in the host machine (not within chroot) to get the right address. Update the IP address to &quot;133.5.166.3&quot;.  Finally <code>/etc/hosts</code> should contain the following entries:</p>
<pre><code>127.0.0.1       localhost
::1             localhost ip6-localhost ip6-loopback
fe00::0         ip6-localnet
ff00::0         ip6-mcastprefix
ff02::1         ip6-allnodes
ff02::2         ip6-allrouters

133.5.166.3 ftp.jp.debian.org  
</code></pre>
<h4 id="0212-apt-get-is-still-failing-"><a class="header" href="#0212-apt-get-is-still-failing-">0.2.1.2 apt-get is still failing !!!</a></h4>
<p>OK, dont worry, download my rootfs from my github repo <a href="https://github.com/teja2010/teja2010.github.io/tree/master/old/extra">here</a>.</p>
<h4 id="03-take-it-for-a-spin"><a class="header" href="#03-take-it-for-a-spin">0.3 Take it for a spin</a></h4>
<p>Finally if all this works out you are ready to start a UML instance.    Just run the following command, where you provide path to rootfs as the value    against &quot;ubda&quot;, and 256MiB as the RAM. DO NOT forget the &quot;M&quot; in 256M, else    UML will try to boot with just to 256Bytes of RAM, and fail :). I usually    provide 256MiB RAM, you can go as low as 100 or 50MiB.</p>
<pre><code>./linux ubda=net_test.rootfs.20150203 mem=256M 
</code></pre>
<p>You will see the VM booting, printing dmesg, bringing up the various kernel    susbsystems.    Finally when a promt to enter the password appears, enter root as the    password. Play around with the tiny VM. When the fun ends, run &quot;halt&quot; to    shutdown UML.</p>
<h4 id="04-attach-gdb"><a class="header" href="#04-attach-gdb">0.4 Attach GDB</a></h4>
<p>It is fun to add breakpoints and view specific code in GDB. To do this,    we have to first find the process id (PID) of the main UML process.    Run the following command to find the UML pid. The output should show multiple PIDs</p>
<pre><code class="language-bash">$ ps aux | grep ubda

0 t teja     27089 12160  2  80   0 - 17629 ptrace 16:52 pts/6    00:00:18 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 S teja     27094 27089  0  80   0 - 17629 read_e 16:52 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 S teja     27095 27089  0  80   0 - 17629 poll_s 16:52 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 S teja     27096 27089  0  80   0 - 17629 poll_s 16:52 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 t teja     27097 27089  0  80   0 -  5206 ptrace 16:52 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 t teja     27288 27089  0  80   0 -  5339 ptrace 16:52 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 t teja     27352 27089  0  80   0 -  4990 ptrace 16:52 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 t teja     27353 27089  0  80   0 -  5294 ptrace 16:52 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 t teja     29405 27089  0  80   0 -  5003 ptrace 16:53 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 t teja     29408 27089  0  80   0 -  5390 ptrace 16:53 pts/6    00:00:00 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
1 t teja     29410 27089  1  80   0 -  5717 ptrace 16:53 pts/6    00:00:08 ./linux ubda=../rootfs_pool/net_test.rootfs.20150203 mem=50M
0 S teja     30278  3682  0  80   0 -  5500 pipe_w 17:05 pts/1    00:00:00 grep --color=auto ubda 
</code></pre>
<p>The third column is the process id and the fourth column is the parent PID.  In the above output PID 27089 starts and then spawns the other threads. Attach gdb to the the main parent thread, which is 27089 in the above example.</p>
<pre><code>sudo gdb -p 27089
</code></pre>
<p>GDB will read the symbols from linux and attach itself. Play around, check    the backtrace, etc. All globals are now accessable.</p>
<h3 id="05-a-private-uml-subnet"><a class="header" href="#05-a-private-uml-subnet">0.5 A private UML subnet</a></h3>
<p>In most cases we want to play around with two UMLs connected to each other    and ignorant of the rest of the world. In such cases, the simplest way is to    connect them using the mcast transport.
Make copies of the rootfs, for each UML instance. In this case have two    copies ready, and run them with one additional argument &quot;eth0&quot;. This will    add an additional eth0 interface. We set eth0 to mcast. i.e. the eth0    interfaces in both the instances are connected over mcast.</p>
<pre><code>./linux ubda=net_test.rootfs.20150203_1 mem=256M eth0=mcast
./linux ubda=net_test.rootfs.20150203_2 mem=256M eth0=mcast 
</code></pre>
<p>Assign addresses to the eth0 interfaces, and they are ready. You try pinging    the other UML. Command to assign address is:</p>
<pre><code>ip address add 192.168.1.1/24 dev eth0  
</code></pre>
<p>The full mcast readme is    <a href="http://user-mode-linux.sourceforge.net/old/text/mcast.txt">here</a>.    It contains details to create more complex mcast networks.</p>
<p>On assigning mcast to a eth device, each UML instance opens sockets which    listen to multicast traffic. If a multicast address is not assigned (like    above), all the instances listen to 239.192.168.1 . All packets are received    and then filtered out based on destination MAC address. The packets    can even be seen in packetdumps collected in the host OS.    Quoting from the above readme, &quot;It's basically like an ethernet    where all machines have promiscuous mode turned on&quot;. Bad for performance,    but very easy to setup.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n2-packet-rx-path-1--basic"><a class="header" href="#n2-packet-rx-path-1--basic">N2. Packet RX path 1 : Basic</a></h1>
<p>This page describes the code flow while receiving a packet. It begins with the packet just entering the core networking code, top half and bottom half processing, basic flow through the IP and UDP layers and finally the packet is enqueued into a socket queue.</p>
<p>Additionally hash based software packet steering across CPUs (RPS and RSS), Inter Processor Interrupts (IPI) and scheduling softirq processing is described. They can be ignored in the first read and can be revisited in later runs after gaining additional context. These sections have been marked OPTIONAL.
NAPI will be described in later pages, all NAPI APIs are ignored now.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n21-n22-top-half-processing"><a class="header" href="#n21-n22-top-half-processing">N2.1-N2.2 Top Half Processing</a></h1>
<h3 id="n21-enter-the-core-top-half-processing"><a class="header" href="#n21-enter-the-core-top-half-processing">N2.1 Enter the Core, top half processing</a></h3>
<p>We assume that the driver has already picked up the packet data and has created a skb. (We will look at how drivers create skbs in the page describing NAPI). This packet needs to be sent up to the core. The kernel provides two simple function calls to do this.</p>
<pre><code>netif_rx()
netif_rx_ni() 
</code></pre>
<p>netif_rx() does two things,</p>
<ol>
<li>
<p>Enqueue the packet into a queue which contains packets that need processing.
The kernel maintains a softnet_data structure for each CPU. It is the core structure that facilitates network processing. Each softnet_data struct contains a &quot;input_pkt_queue&quot; into which packets that need to be processed will be enqueued. This queue is protected by a spinlock that is part of the queue (calls to rps_lock() and rps_unlock() are to lock/unlock the spinlock). The input_pkt_queue is of type sk_buff_head, which is used within by kernel to represent skb lists.
Before enqueuing, if the queue length is more than <code>netdev_max_backlog</code> (whose default length is 1000), the packet is dropped. This value can be modified by changing <code>/proc/sys/net/core/netdev_max_backlog</code>.
For each each packet drop sd-&gt;dropped is incremented. Certain numbers are maintained by softnet_data, I'll add a page describing the struct. TODO</p>
</li>
<li>
<p>After successfully enqueueing the packet, netif_rx schedules softirq processing if it has not already been scheduled. The next section describes how softirq is scheduled.</p>
</li>
</ol>
<p>Parts of the code have been added below. All the core networking functions are described in <code>net/core/dev.c</code>.</p>
<pre><code class="language-c">netif_rx()
{
    netif_rx_internal()
    {
        enqueue_to_backlog()
        {
            // checks on queue length
            rps_lock(sd);
            __skb_queue_tail(&amp;sd-&gt;input_pkt_queue, skb);
            rps_unlock(sd);

            ____napi_schedule(sd, &amp;sd-&gt;backlog)
            {
                // if not scheduled already schedule softirq
                __raise_softirq_irqoff(NET_RX_SOFTIRQ);
            }
        }
    }

}
</code></pre>
<p>netif_rx() and netif_rx_ni() are very similar except the later additionally begins softirq processing immediately, which will be explained in subsequent section.</p>
<p>This ends the top half processing, bottom half was scheduled, which will undertake rest of the packet processing.</p>
<h3 id="n22-softirqs-softirq-scheduling-optional"><a class="header" href="#n22-softirqs-softirq-scheduling-optional">N2.2 Softirqs, Softirq Scheduling [OPTIONAL]</a></h3>
<p>One detail that I have ignored in the previous discussion is to specify which CPU the top and bottom half actually run.</p>
<p>The top half runs in the hardware interrupt (IRQ) context, which is a kernel thread which can be on any CPU. (This description is not really true, I have a separate page planned on interrupts where this will be described in more detail. Till then this way of visualizing it is not really wrong). Say it runs on CPU0, i.e. netif_rx() is called on CPU0. The packet will be enqueued onto CPU0's input_packet_queue. The kernel thread will then schedule softirq processing on CPU0 and exit. The bottom half will also run on the same CPU. This section describes how the bottom half is scheduled by the top half and how softirq begins.</p>
<p>Just Google what hard interrupts (IRQ) and soft interrupts (softirq) are for some background. Hardware interrupts (IRQs) will stop all processing. For interrupts that take very long to run, some work is done in IRQ and the rest is done in a softirq. Packet processing is one such task that takes very long to complete so NET_RX is the corresponding soft interrupt which takes care of packet processing.</p>
<p>Linux currently has the following softirqs declared in <code>include/linux/interrupt.h</code></p>
<pre><code class="language-c">enum
{
    HI_SOFTIRQ=0,
    TIMER_SOFTIRQ,
    NET_TX_SOFTIRQ,
    NET_RX_SOFTIRQ,
    BLOCK_SOFTIRQ,
    IRQ_POLL_SOFTIRQ,
    TASKLET_SOFTIRQ,
    SCHED_SOFTIRQ,
    HRTIMER_SOFTIRQ, /* Unused */
    RCU_SOFTIRQ,

    NR_SOFTIRQS

}; 
</code></pre>
<p>The names are mostly indicative of the subsystem each softirq serves. During kernel initialization, a function is registered for each of these softirqs. When softirq processing is needed, this function is called.
For example after core networking init is done, <code>net_dev_init()</code> registers <code>net_rx_action</code> and <code>net_tx_action</code> as the functions corresponding to <code>NET_RX</code> and <code>NET_TX</code>.</p>
<pre><code class="language-c">  open_softirq(NET_TX_SOFTIRQ, net_tx_action);
  open_softirq(NET_RX_SOFTIRQ, net_rx_action); 
</code></pre>
<p>A softirq is scheduled by calling <code>raise_softirq()</code>, which internally disables irqs and calls <code>__raise_softirq_irqoff()</code>. This function sets a bit corresponding to the softirq in the percpu bitmask <code>irq_stat.__softirq_pending</code>. The bitmask is used to track all pending softirqs on the CPU. This operation must be done with all interrupts are disabled on the CPU. Without interrupts disabled, the same bitmask can be overwritten by another interrupt handler, undoing the current change.</p>
<p>Grepping for <code>ksoftirqd</code> in ps, should show multiple threads, one for each CPU. These are threads spawned during init to process pending softirqs on each of the CPUs. Periodically the scheduler will allow ksoftirqd to run, and if any of the bits are set, it's registered function is called.</p>
<p>During packet RX, <code>net_rx_action()</code> is called.</p>
<p>The CFS scheduler makes sure that all threads get their fair share of CPU time. So ksoftirqd and an application thread will both get their fair share. But, during softirq processing, ksoftirqd disables all irqs and the scheduler has no way of interrupting the thread. Hence all registered functions are have checks to prevent the ksoftirqd from high-jacking the CPU for too long.
In this case, a buggy <code>net_rx_action()</code> function may be able to push packets into the socket queue but the application never will never get a chance to actually read the packets.</p>
<p>To conclude, <code>netif_rx()</code> calls <code>__raise_softirq_irqoff(NET_RX_SOFTIRQ)</code> to schedule softirq processing on the current CPU. The ksoftirqd running on the current CPU will check the bitmask, since <code>NET_RX_SOFTIRQ</code> is pending will call <code>net_rx_action()</code></p>
<h4 id="n221-run-softirq-now"><a class="header" href="#n221-run-softirq-now">N2.2.1 Run Softirq Now</a></h4>
<p>Other than softirq being scheduled by the scheduler, it is sometimes necessary to kick start processing. For example when a sudden burst of packets arrive, due to delays in softirq processing, packets might be dropped. In such cases, when a burst is detected, kick-starting packet processing is helpful. In the above example, calling <code>netif_rx_ni()</code> will kick start packet processing, in addition to enqueuing the packet .</p>
<h3 id="n23-packet-steering-rss-and-rps-optional"><a class="header" href="#n23-packet-steering-rss-and-rps-optional">N2.3 Packet Steering (RSS and RPS) [OPTIONAL]</a></h3>
<p>RSS and RPS are techniques that help with scaling packet processing across multiple CPUs. They allow distribution of packet processing across CPUs, while restricting a flow to a single CPU. i.e. each flow is assigned a CPU and flows are distributed across CPUs.</p>
<h4 id="n231-packet-flow-hash"><a class="header" href="#n231-packet-flow-hash">N2.3.1 Packet flow hash</a></h4>
<p>Firstly To identify packet flows, a hash is computed based on the following 4-tuple.
(source address, destination address, source port, destination port).
For certain protocols that do not support ports, a tuple containing just the source and destination addresses is used to compute the hash.
The hash is computed in <code>__skb_get_hash()</code>. After computing the hash, it is updated in skb-&gt;hash.
Some drivers have the hardware to offload hash computation, which is then set by the driver before passing the packet to the core networking layer.</p>
<h4 id="n232-rss-receive-side-scaling"><a class="header" href="#n232-rss-receive-side-scaling">N2.3.2 RSS: Receive Side Scaling</a></h4>
<p>RSS acheives packet steering by configuring receive queues (usually one for each CPU), and by configuring seperate interrupts for each queue and pinning the interrupts to the specific CPU. On receiving a packet, based on it's hash the packet is put in the right queue and the corresponding interrupt is raised.</p>
<h3 id="n233-rps-receive-packet-steering"><a class="header" href="#n233-rps-receive-packet-steering">N2.3.3 RPS: Receive Packet Steering</a></h3>
<p>RPS is RSS in software. While pushing the packet into the core network through <code>netif_rx()</code> or <code>netif_receive_skb()</code>, a CPU is chosen for the packet based on the <code>skb-&gt;hash</code>. The packet is then enqueued into the target CPU's input_packet_queue. Since the operation must have all interrupts disabled, a softirq cannot be directly scheduled on different core. So an Inter Processor Interrupt is used to schedule softirq processing on the other core.</p>
<p>After RSS decides to put the packet on a remote core, in the <code>rps_ipi_queued()</code> function, the target CPU's softnet struct is added to the current core's <code>sd-&gt;rps_ipi_next</code> which is a list to softnet structs for which an IPI has to be sent. During the current core's softirq processing, all the accumulated IPIs are sent to those cores by traversing the rps_ipi_next list.</p>
<p>IPIs are actually sent by scheduling a job on a remote core by  calling <code>smp_call_function_single_async()</code> during NET_RX processing.</p>
<pre><code class="language-c">netif_rx_internal(skb)
{
    cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow);
    enqueue_to_backlog(skb, cpu)
    {
        sd = &amp;per_cpu(softnet_data, cpu);    //get remote cpu sd
        __skb_queue_tail(&amp;sd-&gt;input_pkt_queue, skb); //enqueue
        rps_ipi_queued(sd);        //add sd to rps_ipi_next
    }
} 
</code></pre>
<p>Kernel documentation describes these methods and also provides instructions to configure them.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n24-n25-bottom-half-processing"><a class="header" href="#n24-n25-bottom-half-processing">N2.4-N2.5 Bottom Half Processing</a></h1>
<h3 id="n24-softirq-processing-net_rx"><a class="header" href="#n24-softirq-processing-net_rx">N2.4 Softirq processing NET_RX</a></h3>
<p>Softirq processing was scheduled by the bottom half, and NET_RX begins. All of NET_RX processing is done in <code>net_rx_action()</code>. The logic is to process packets until one of the following events occurs:</p>
<ol>
<li>The packet queue is empty. In this case NET_RX softirq stops.</li>
<li>NET_RX has been running for longer than <code>netdev_budget_usecs</code>, whose default value is 2 milliseconds.</li>
<li>NET_RX has processed more than <code>netdev_budget</code> (fixed value of 300) packets. (We will revist this constraint while looking at NAPI)</li>
</ol>
<p>In cases 2 and 3, there still might be packets to process, in which case NET_RX will schedule itself before exiting, (i.e. set the NET_RX_SOFTIRQ bit before exiting), so it can process some more packets in another session. In cases 2 and 3 NET_RX processing is almost at it's limits. To indicate this <code>sd-&gt;time_squeeze</code> is incremented, so that a few parameters can be tuned. We will revisit this while discussing NAPI.</p>
<p>Softirq processing is done with elevated privilages, which can easily cause it to high-jack the complete CPU. The above constraints are to make sure that softirq processing allows the applications run. If the softirq were to high-jack the CPU, the user application would never run, and the end user would see applications not responding.</p>
<p>The actual function that dequeues packets from <code>input_pkt_queue</code> and begins processing them is <code>process_backlog()</code>. After dequeueing the packet it calls <code>__netif_receive_skb()</code> which pushes the packet up into the protocol stacks.</p>
<p>For now ignore the napi part of <code>net_rx_action()</code>, it calls <code>napi_poll()</code> which will call the registered poll function <code>n-&gt;poll()</code>. The poll function is set to process_backlog. For now believe me even if it does not make much sense. It will make sense one we look at the NAPI framework.</p>
<pre><code class="language-c">net_rx_action()
{
    unsigned long time_limit = jiffies +
                               usecs_to_jiffies(netdev_budget_usecs);
    int budget = netdev_budget;

    budget -= napi_poll(n, &amp;repoll)
    {
        work = n-&gt;poll(n, weight) // same as process_backlog
        process_backlog(n, weight)
        {
            skb_queue_splice_tail_init(&amp;sd-&gt;input_pkt_queue,
                                       &amp;sd-&gt;process_queue);
            while ((skb = __skb_dequeue(&amp;sd-&gt;process_queue)))
                __netif_receive_skb(skb);
        }
    }
    // time and budget constraints
    if (unlikely(budget &lt;= 0 ||
                 time_after_eq(jiffies, time_limit))) {
        sd-&gt;time_squeeze++;
        break;
    }
} 
</code></pre>
<h3 id="n25-__netif_receive_skb_core"><a class="header" href="#n25-__netif_receive_skb_core">N2.5 __netif_receive_skb_core()</a></h3>
<p><code>__netif_receive_skb()</code> internally calls <code>__netif_receive_skb_core()</code> for the main packet processing. <code>__netif_receive_skb_core()</code> is large function which handles multiple ways in which a packet can be processed. This section tries to cover some of them.</p>
<ol>
<li>
<p>skb timestamp:
<code>skb-&gt;tstamp</code> field is filled with the time at which the kernel began processing the packet. This information is used in various protocol stacks. One example is it's usage by AF_PACKET, which is the protocol tools like wireshark use to collect packet dumps. AF_PACKET extracts the timestamp from <code>skb-&gt;tstamp</code> and provides to userspace as part of struct tpacket_uhdr. This timestamp is the one that wireshark reports as the time at which the packet was received.</p>
</li>
<li>
<p>Increment softnet stat:
<code>sd-&gt;processed</code> is incremented, which is representative of the number of packets that were processed on a particular core. The packets might be dropped by the kernel for various reasons later, but they were processed on a particular core.</p>
</li>
<li>
<p>packet types:
At this point the packet is sent to all modules that want to process packets. The list of packet types that the kernel supports is defined in <code>include/linux/netdevice.h</code>, just above PTYPE_HASH_SIZE macro definition. Other than the ones described above, promiscuous packet types (processes packets irrespective of protocol) like AF_PACKET and custom packet_types added by various drivers and subsystems are all supported. Each of them fill up a packet_type structure and register it by calling <code>dev_add_pack()</code>. Based on the type and netdevice the struct is added to the respective packet_type list. <code>__netif_receive_skb()</code> based on the skb's protocol and netdevice traverses the particular list, delivering the packet by calling <code>packet_type-&gt;func()</code>.
All registered packet_types can be seen at <code>/proc/net/ptype</code>.</p>
</li>
</ol>
<pre><code class="language-bash">$ cat /proc/net/ptype

Type Device      Function
ALL           tpacket_rcv
0800          ip_rcv
0011          llc_rcv [llc]
0004          llc_rcv [llc]
0806          arp_rcv
86dd          ipv6_rcv 
</code></pre>
<p>The ptype lists are described below:</p>
<ol>
<li>
<p><code>ptype_all</code>: It is a global variable containing promiscuous packet_types irrespective of netdevice. Each AF_PACKET socket adds a packet_type to this list. packet_rcv() is called to pass the packet to userspace.</p>
</li>
<li>
<p><code>skb-&gt;dev-&gt;ptype_all</code>: Per netdevice list containing promiscuous packet_types specific to the netdevice. TODO: find an example.</p>
</li>
<li>
<p><code>ptype_base</code>: It is a global hash table, with key as the last 16bits of <code>packet_type-&gt;type</code> and value as a list of packet_type. For example <code>ip_packet_type</code> will be added to ptype_base[00], with <code>ptype-&gt;func</code> set to ip_rcv. While traversing, based on <code>skb-&gt;protocol</code> 's last 16bits, a list is chosen and the packet is delivered to all packet_types whose type matches skb-&gt;protocol.</p>
</li>
<li>
<p><code>skb-&gt;dev-&gt;ptype_specific</code>: Per netdevice list containing protocol specific packet types. The packet is delivered to if the skb-&gt;protocol matches ptype_type. Mellanox for example adds a <code>packet_type</code> with type set to <code>ETH_P_IP</code>, to process all UDP packets received by the driver. See <code>mlx5e_test_loopback()</code>. The name suggests some sort of loopback test. I am not really sure how. IDK.</p>
<p>One important detail is that the same packet will be sent to all applicable packet_type-s. Before delivering the skb, <code>skb-&gt;users</code> is incremented. <code>skb-&gt;users</code> is the number users that are (ONLY) reading the packet. Each module after completing necessary processing call <code>kfree_skb()</code>, which will first decrement users, and then free the skb only if skb-&gt;users hits zero. So the same skb pointer is shared by all the modules, and the last user will free the skb.</p>
</li>
<li>
<p>RX handler:
Drivers can register a rx handler, which will be called if a packet is received on the device. The <code>rx_handler</code> can return values based on which packet processing can stop or continue. If RX_HANDLER_CONSUMED is returned, the driver has completed processing the packet and <code>__netif_receive_skb_core()</code> can stop processing further. If RX_HANDLER_PASS is returned, skb processing will continue. The other values supported and ways to register/unregister a rx handler are available in <code>include/linux/netdevice.h</code> , above enum rx_handler_result.
For example if the driver wants to support a custom protocol header over IP, a rx handler can be registered which will process the outer header and return RX_HANDLER_PASS. Futher IP processing can continue when the packet is delivered to ip_packet_type. Note that the packet dumps collected will still contain the custom header. (It is actually better to return RX_HANDLER_CONSUMED and enqueue the packet by calling netif_receive_skb. This will allow the driver to run the packet through GRO offload engine and to distribute packet processing with RPS. Ignore this comment for now.)</p>
</li>
<li>
<p>Ingress Qdisc processing. We will look at it in a different page, after we have looking at Qdiscs and TX. Similar to RX handler, certain registered functions run on the packet and based on the return value, the processing can stop or continue. But unlike a RX handler, the functions to run are added from userspace.</p>
</li>
</ol>
<p>The order in which the <code>__netif_receive_skb_core()</code> delivers (if applicable) the packets is:</p>
<ul>
<li>Promiscuous packet_type</li>
<li>Ingress qdisc</li>
<li>RX handler</li>
<li>Protocol specific packet_type</li>
</ul>
<p>Finally if if none of them consume the packet, the packet is dropped and netdevice stats are incremented.</p>
<pre><code class="language-c">__netif_receive_skb_core()
{
    net_timestamp_check(!netdev_tstamp_prequeue, skb)
    {
        __net_timestamp(SKB);
    }

    __this_cpu_inc(softnet_data.processed);
    
    //Promiscuous packet_type
    list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {
        if (pt_prev)
            ret = deliver_skb(skb, pt_prev, orig_dev);
        pt_prev = ptype;
    }
    list_for_each_entry_rcu(ptype, &amp;skb-&gt;dev-&gt;ptype_all, list) {
        if (pt_prev)
            ret = deliver_skb(skb, pt_prev, orig_dev);
        pt_prev = ptype;
    }
    
    //Ingress qdisc
    skb = sch_handle_ingress(skb, &amp;pt_prev, &amp;ret, orig_dev);
    
    //RX handler
    rx_handler = rcu_dereference(skb-&gt;dev-&gt;rx_handler);
    switch (rx_handler(&amp;skb)) {
    case RX_HANDLER_CONSUMED:
        ret = NET_RX_SUCCESS;
        goto out;
    case RX_HANDLER_PASS:
        break;
    default:
        BUG();
    }
    
    //Protocol specific packet_type
    deliver_ptype_list_skb(skb, &amp;pt_prev, orig_dev, type,
                           &amp;ptype_base[ntohs(type) &amp; PTYPE_HASH_MASK]);
    deliver_ptype_list_skb(skb, &amp;pt_prev, orig_dev, type,
                           &amp;skb-&gt;dev-&gt;ptype_specific);

} 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n26-n27-ip-and-udp-processing"><a class="header" href="#n26-n27-ip-and-udp-processing">N2.6-N2.7 IP and UDP Processing</a></h1>
<h3 id="n26-ip-layer"><a class="header" href="#n26-ip-layer">N2.6 IP layer</a></h3>
<p>Assuming the skb is an IP packet, the skb will enter <code>ip_rcv()</code>, which then calls <code>ip_rcv_core()</code>. <code>ip_rcv_core</code> validates the IP header (checksum, checks on ip header length, etc), updates ip stats and based on the transport header set in the IP header will set <code>skb-&gt;transport_header</code>.</p>
<p>The protocol stacks maintain counts when packets enter and counts of the number of packets that were dropped. These numbers can be seen at <code>/proc/net/snmp</code>. The correspnding enum can be found at <code>include/uapi/linux/snmp.h</code>.</p>
<pre><code>ip_rcv_core()
{
    __IP_UPD_PO_STATS(net, IPSTATS_MIB_IN, skb-&gt;len);

    iph = ip_hdr(skb);
    
    if (unlikely(ip_fast_csum((u8 *)iph, iph-&gt;ihl)))
        goto csum_error;
    
    skb-&gt;transport_header = skb-&gt;network_header + iph-&gt;ihl*4;
    return skb;

csum_error:
    __IP_INC_STATS(net, IPSTATS_MIB_CSUMERRORS);
    return NULL;
}
</code></pre>
<p>ip_rcv then sends the packet through the netfilter PREROUTING chain. The netfilter subsystem allows the userspace to filter/modify/drop packets based on the packet's attributes. Tools iptables/ip6tables are used to add/remove IP/IPv6 rules. The netfilter subsystem contains 5 chains, PREROUTING, INPUT, FORWARD, OUTPUT and POSTROUTING. Each chain contains rules and corresponding actions. If a rule matches a packet, the corresponding action is taken. We will look at them in a separate page dedicated to iptables. An easy example is that it is used to act as a firewall to drop unwanted traffic.</p>
<pre><code>	                transport layer (TCP/UDP)

   ip_local_deliver_finish()
	        🠕                                   |
	      INPUT                               OUTPUT
	        |                                   🠗
	ROUTING DECISION  -----  FORWARDING  -----  +
	        🠕                                   |
	    PREROUTING                         POSTROUTING
	        |                                   🠗
	     ip_rcv()

	                     CORE NETWORKING 
</code></pre>
<p>While receiving a packet, at the end of <code>ip_rcv()</code> it enters the PREROUTING chain, at the end of which it enters <code>ip_rcv_finish()</code>. Based on the packet's ip address, a routing decision is taken if the packet should be locally consumed or if it is to forwarded to a different system. (I'll describe this in more detail in a separate page). If the packet should be locally consumed <code>ip_local_deliver()</code> is called. The packet then enters the INPUT chain, and finally comes out at <code>ip_local_deliver_finish()</code>.</p>
<p>Based on the protocol set in the IP header, the corresponding protocol handler is called. If a transport protocol is supported over IP, the corresponding handler is registered by calling inet_add_protocol().</p>
<p>Yes, this section skips a lot of content, I'll add a separate sections for IP processing, netfilter (esp. nftables) and routing.</p>
<h3 id="n27-udp-layer"><a class="header" href="#n27-udp-layer">N2.7 UDP layer</a></h3>
<p>If the packet is an UDP packet, <code>udp_rcv</code> is the protocol handler called, which internally calls <code>__udp4_lib_rcv()</code>. First the packet header is validated, pseudo ip checksum is checked and then if the packet is unicast, based on the port numbers the socket is looked up, and then <code>udp_unicast_rcv_skb()</code> is called, which then calls <code>udp_queue_rcv_skb()</code>.</p>
<p><code>udp_queue_rcv_skb()</code> checks if the udp socket has a registered function to handle encapsulated packets. If the handler is found the corresponding handler is called, which processes the packet further. For example in case of XFRM encapsulation <code>xfrm4_udp_encap_rcv()</code> is registered as the handler. (XRFM short for transform, adds support to add encrypted tunnels in the kernel).</p>
<p>If no encap_rcv handler is found, full udp checksum is done and <code>__udp_queue_rcv_skb()</code> is called. Internally it calls <code>__udp_enqueue_schedule_skb()</code> which checks if the sk memory is sufficient to add the packet and then calls <code>__skb_queue_tail()</code> to enqueue the packet into <code>sk_receive_queue</code>. If the application has called the recv() system call and is waiting for the packet the process moves to __TASK_STOPPED state and the scheduler no longer schedules it. <code>sk-&gt;sk_data_ready(sk)</code> is called so that it's state is set to TASK_INTERRUPTIBLE, and the scheduler then schedules the application. On waking up, the packet is dequeued from the queue and the application recv()s the packet data. Receiving a packet and socket calls will be described in a separate page.</p>
<pre><code class="language-c">__udp4_lib_rcv()
{
    uh   = udp_hdr(skb);
    if (udp4_csum_init(skb, uh, proto)) //pseudo ip csum
        goto csum_error;

    sk = __udp4_lib_lookup_skb(skb, uh-&gt;source, uh-&gt;dest, udptable);
    if (sk) {
        return udp_unicast_rcv_skb(sk, skb, uh)
        {
            ret = udp_queue_rcv_skb(sk, skb);
                  //continued below..
        }
    }

}

udp_queue_rcv_skb(sk, skb)
{
    struct udp_sock *up = udp_sk(sk);

    encap_rcv = READ_ONCE(up-&gt;encap_rcv);
    if (encap_rcv) {
        if (udp_lib_checksum_complete(skb))
            goto csum_error;
    
        ret = encap_rcv(sk, skb);
    }
    
    udp_lib_checksum_complete(skb);
    return __udp_queue_rcv_skb(sk, skb)
    {
        rc = __udp_enqueue_schedule_skb(sk, skb)
        {
    
            rmem = atomic_read(&amp;sk-&gt;sk_rmem_alloc);
            if (rmem &gt; sk-&gt;sk_rcvbuf)
                goto drop;
    
            __skb_queue_tail(&amp;sk-&gt;sk_receive_queue, skb);
            sk-&gt;sk_data_ready(sk);
            // == sock_def_readable()
        }
    }

} 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n2-packet-tx-path-1--basic"><a class="header" href="#n2-packet-tx-path-1--basic">N2. Packet TX path 1 : Basic</a></h1>
<p>This page contains the basic code flow while transmitting a packet. It    begins with the userspace sendmsg, enters the UDP and IP stacks, finds    a route, enters core networking and finally being handed over to the driver    which pushes    it out. TX, unlike RX, can happen without a softirq being raised. The    processing happens completely in the application context. In this page I    describe packet transmission without a softirq being raised. I'll cover    how qdiscs are used in a separate page, after which NET_TX with qdiscs    will be described.</p>
<p>UDP &amp; IP stack processing and routing will be described in detail in a    separate page, this is just a basic overview.    We will end the discussion by handling over the packet to the driver. How    the driver actually transmits the packet will be described in later pages.  </p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n31-32-sendmsg-from-userspace"><a class="header" href="#n31-32-sendmsg-from-userspace">N3.1-3.2 sendmsg() from userspace</a></h1>
<h3 id="31-sendmsg"><a class="header" href="#31-sendmsg">3.1 sendmsg()</a></h3>
<p>After opening a UDP socket the application gets a fd as a handle for the  underlying kernel socket. The application sends data into the socket by calling <code>send()</code> or <code>sendmsg()</code> or <code>sendto()</code>, all of which will send out UDP data.</p>
<p>​      On calling the <code>sendmsg()</code> system call, the syscall trap will save the application's process context and switch to running the kernel code. Kernel code runs in the application context, i.e. any traces that print the pid of the process will return the application's pid. If you do not know this, believe me for now, signals &amp; syscalls are explained in a  separate page. The kernel registers functions that are run for each system call. The registered function's name is <code>__do_sys_</code> + <code>syscall</code> name. In this case the function is <code>__do_sys_sendmsg()</code>, which internally      calls <code>__sys_sendmsg()</code>. The arguments passed to the call are the <code>fd</code>, the  <code>msghdr</code> struct and flags.</p>
<p>The first step is to get the kernel socket from the fd. Each fd the application  holds is a handle to a kernel socket. The socket can be for an open file, a  UDP socket, UNIX socket, etc. The socket is usually represented  with the var <code>sock</code>. <code>___sys_sendmsg</code> is called with the sock, msghdr and      flags. It simply checks if the arguments are valid, copies the msghdr      (allocated in userspace) into kernel memory and then calls <code>sock_sendmsg()</code>.</p>
<p>sock_sendmsg() checks if the application is allowed to proceed. Linux      has kernel modules like SELinux and AppArmour which audit each system      call and based on the configured rules allow or reject the system call.      If <code>security_socket_sendmsg()</code> does not return any errors, <code>sock_sendmsg_nosec()</code>      is called, which internally calls the <code>sock-&gt;ops-&gt;sendmsg()</code>.      socket ops are registered during socket creation based on the protocol family (<code>AF_INET</code>, <code>AF_INET6</code>, <code>AF_UNIX</code>) and socket type (<code>SOCK_DGRAM</code>, <code>SOCK_STREAM</code>).      Since a udp socket is a SOCK_DGRAM socket of AF_INET family the ops      registered are <code>inet_dgram_ops</code>, defined in <code>net/ipv4/af_inet.c</code>. And      <code>sock-&gt;ops-&gt;sendmsg()</code> is <code>inet_sendmsg()</code>.</p>
<pre><code class="language-c">SYSCALL_DEFINE3(sendmsg, int, fd, struct user_msghdr __user *, msg,
                unsigned int, flags)
{
    return __sys_sendmsg(fd, msg, flags) {
        sock = sockfd_lookup_light(fd, &amp;err, &amp;fput_needed);

        err = ___sys_sendmsg(sock, msg, &amp;msg_sys, flags) {

            //copy msg (usr mem) into msg_sys (kernel mem)
            err = copy_msghdr_from_user(msg_sys, msg, NULL, &amp;iov);
            sock_sendmsg(sock, msg_sys) {
                int err = security_socket_sendmsg();
                if(err)
                    return;

                sock_sendmsg_nosec(sock, msg_sys) {
                    sock-&gt;ops-&gt;sendmsg(sock, msg_sys);
                             //inet_sendmsg();
                }
            }
        }
    }
} 
</code></pre>
<h3 id="22-udp--ip-sendmsg"><a class="header" href="#22-udp--ip-sendmsg">2.2 UDP &amp; IP sendmsg</a></h3>
<p>At this point we will begin using the networking struct sock (different from a <code>socket</code>), represented usually with the var <code>sk</code>. Each socket will either have a valid sock or a file. In our case a <code>sock-&gt;sk</code> will contain a valid sock. We check if socket needs to be bound to a ephemeral port, and then call      <code>sk-&gt;sk_prot-&gt;sendmsg()</code>. During socket creation, the sock is added to the socket, and protocol handlers are registered to the sock. In this case,      for a UDP socket, <code>sk_prot</code> is set to <code>udp_prot</code> (defined in <code>net/ipv4/udp.c</code>).      And <code>sk_prot-&gt;sendmsg</code> is set to <code>udp_sendmsg()</code>. The arguments      have not changed, we will pass sk and msghdr.</p>
<p>​      Till this point we have not begun constructing the packet, the focus was more on socket options. <code>udp_sendmsg</code> will first extract the destination address      (usually variable <code>daddr</code>) and dest port (usually  <code>dport</code>), from  the <code>msghdr-&gt;msg_name</code>. The source port is extracted from the sock. This information is passed to find a route for the packet. The first time a      packet is sent out of a sock, the route has to be found by going through      the routing tables. This route result is saved in <code>sk-&gt;sk_dst_cache</code>,      which is used for packets that are sent later. At this point the packet's      source address is extracted from the route. All the details about the      packet's flow are saved in <code>struct flowi4</code>, which are saddr, daddr, sport,      dport, protocol, tos (type of service), sock mark, UID (user identifier),      etc. We now have all the information, the addresses, ports and certain      information to fill in the IP header with. We can begin filling in the      packet. <code>ip_make_skb()</code> will create the skb, and the skb will be sent out by calling <code>udp_send_skb()</code>.</p>
<pre><code class="language-c">int inet_sendmsg(struct socket *sock, struct msghdr *msg, size_t size)
{
    inet_autobind(sk)
    DECLARE_SOCKADDR(struct sockaddr_in *, usin, msg-&gt;msg_name);

    daddr = usin-&gt;sin_addr.s_addr; // get daddr from msghdr
    dport = usin-&gt;sin_port;

    rt = (struct rtable *)sk_dst_check(sk, 0);
    if (!rt) {
        flowi4_init_output();
        // pass daadr, dport...
        rt = ip_route_output_flow(net, fl4, sk);

        sk_dst_set(sk, dst_clone(&amp;rt-&gt;dst));
        //next time sendmsg is called, sk_dst_check() will return the rt
    }

    saddr = fl4-&gt;saddr; // route lookup complete, saddr is known

    skb = ip_make_skb(); // create skb(s)
    err = udp_send_skb(skb, fl4, &amp;cork); // send it to ip layer
} 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n33-34-alloc-skb-and-send_skb"><a class="header" href="#n33-34-alloc-skb-and-send_skb">N3.3-3.4 alloc skb and send_skb</a></h1>
<h3 id="n33-alloc-skb-fill-it-up-optional"><a class="header" href="#n33-alloc-skb-fill-it-up-optional">N3.3 Alloc skb, fill it up (OPTIONAL)</a></h3>
<p><code>ip_make_skb()</code> is called to create a skb and is provided flowi4, sk, msg ptr, msg length and route as  the arguments. This is a generic function that can be called from any tranport layer, here UDP calls it and the argument <code>tranhdrlen</code> (transport header length) is equal to sizeof an udp header. Additionally, the length field is equal to the amount of data below the      ip header, i.e. length contains payload length plus udp header length. </p>
<p>A skb queue head is inited, which      contains a list of skbs. Note that a head itself DOES NOT contain data. It      is a handle to a skb list. On init, the list is empty, with <code>sk_buff_head-&gt;next</code>      equal to <code>sk_buff_head-&gt;prev</code> equal to its own address, and <code>sk_buff_head-&gt;qlen</code>      is zero. Multiple skbs will be added to it if the msg size is greater      than the MTU, forcing IP fragmentation. For now, we will ignore ip fragmentation, so a single skb will be added to it later. </p>
<p>​      Next <code>__ip_append_data()</code> is called to fill in the queue with the skb(s).      The primary goal of <code>__ip_append_data</code> is to estimate memory necessary for  the packet(s) and accordingly create and enqueue skb(s) into the queue.  The skb needs memory necessary to accommodate:</p>
<ol>
<li>link layer headers: each device during init sets <code>dev-&gt;hh_len</code>. Hardware header length (usually represented by var <code>hh_len</code>) is the maximum space the driver will need to fill in header(s) below the network header. e.g. ethernet header is added by ethernet drivers.</li>
<li>IP and UDP headers. The function also handles the case where the packet needs IP fragmentation. In that case, additional logic to allocate memory for fragmentation headers is necessary.</li>
<li>Payload. Obviously.</li>
</ol>
<p>Additionally some extra tail space is also      provided while allocating the skb. The allocation logic is shown in the      code below. Once  the calculation is done, <code>sock_alloc_send_skb()</code> is called, which internally  calls <code>sock_alloc_send_pskb()</code> to allocate the skb. Each skb must be accounted for in the sock where it was created (TX) or in the sock where it is destined to (RX). This is to control the memory used by packets. Each sock will have restrictions on the amount of memory it can use. In this case <code>wmem</code>, the amount of data written into the socket that hasn't transmitted yet, is a constraint      while allocating data. If      wmem is full, <code>sendmsg()</code> call can get blocked (unless the socket is set to non blocking mode) till sock memory is freed. Once data is allocated, the  udp payload data is written into the skb. <code>skb-&gt;transport_header</code> and <code>skb-&gt;network_header</code> are set. IP and UDP headers haven't been filled yet, but pointers to where they have to be  filled are set. The skb is added to the skb queue and finally sock wmem is updated.</p>
<p>Next __ip_make_skb() will fill in ip header. (ignore fragmentation code for now, which will run if the queue has more than one skb). Finally it returns the created skb's pointer. </p>
<pre><code class="language-c">struct sk_buff *ip_make_skb()
{
    struct sk_buff_head queue;
    __skb_queue_head_init(&amp;queue);

    err = __ip_append_data()
    {
        hh_len = LL_RESERVED_SPACE(rt-&gt;dst.dev);

        fragheaderlen = sizeof(struct iphdr) + (opt ? opt-&gt;optlen : 0);
        // opt is NULL, fragheaderlen is equal to sizeof(struct iphdr)

        datalen = length + fraggap; //fraggap is zero
        // length = udphdr len + payload length
        fraglen = datalen + fragheaderlen;

        alloclen = fraglen;
        skb = sock_alloc_send_skb(sk,
                alloclen + hh_len + 15,
                (flags &amp; MSG_DONTWAIT), &amp;err);// ------- Step 0

        skb_reserve(skb, hh_len);  // -------------------------- Step 1
        data = skb_put(skb, fraglen + exthdrlen - pagedlen);
        // exthdrlen &amp; pagedlen are zero.  --------------------- Step 2
        skb_set_network_header(skb, exthdrlen);
        skb-&gt;transport_header = (skb-&gt;network_header +
                fragheaderlen); // --------------------- Step 3

        data += fragheaderlen + exthdrlen; // ------------------ Step 4
        // move pointer to where payload starts
        copy = datalen - transhdrlen - fraggap - pagedlen;
        // amount of payload data that needs to be copied.
        // datalen = payload len + udp header len.
        // transhdrlen = udp header len

        getfrag(from, data + transhdrlen, offset, copy, fraggap, skb);
        // getfrag is set to ip_generic_getfrag()
        {    //copy and update csum
            csum_and_copy_from_iter_full(to, len, &amp;csum, &amp;msg-&gt;msg_iter);
            skb-&gt;csum = csum_block_add(skb-&gt;csum, csum, odd);
        }

        length -= copy + transhdrlen; // copied length is subtracted

        skb-&gt;sk = sk;
        __skb_queue_tail(queue, skb);

        refcount_add(wmem_alloc_delta, &amp;sk-&gt;sk_wmem_alloc);
    }

    return __ip_make_skb(sk, fl4, &amp;queue, cork)
    {
        skb = __skb_dequeue(queue);

        iph = ip_hdr(skb);
        iph-&gt;version = 4;
        iph-&gt;ihl = 5;
        iph-&gt;tos = (cork-&gt;tos != -1) ? cork-&gt;tos : inet-&gt;tos;
        iph-&gt;frag_off = df;
        iph-&gt;ttl = ttl;
        iph-&gt;protocol = sk-&gt;sk_protocol;
        ip_copy_addrs(iph, fl4); // copy addresses from flow info
    }
} 
</code></pre>
<h4 id="n331-older-skb-allocation-logic"><a class="header" href="#n331-older-skb-allocation-logic">N3.3.1 (Older?) Skb allocation logic</a></h4>
<p>The figure below which shows how pointers in skb meta data are being updated corresponding to steps commented in the code above. These pictures are from  <a href="http://vger.kernel.org/%7Edavem/skb_data.html">davem's skb_data page</a> which describes udp packet data being filled in a skb. This logic is very different from what I have described above. It is possible that this was the allocation logic earlier. It is entirely possible I have missed something. Comments are welcome.</p>
<p><img src="N/imgs/02_skb_creation2.svg" alt="skb_creation" /></p>
<h4 id="34-udp-and-ip-send_skb"><a class="header" href="#34-udp-and-ip-send_skb">3.4 UDP and IP send_skb</a></h4>
<p>​      <code>udp_send_skb()</code> is simple, it fills in the UDP header, computes the checksum (<code>csum</code>),      and sends the skb to <code>ip_send_skb()</code>. If ip_send_skb returns an error,      SNMP value SNDBUFERRORS is incremented. And if everything goes well OUTDATAGRAMS      is incremented.</p>
<p><code>ip_send_skb()</code> calls <code>ip_local_out()</code>, which calls <code>__ip_local_out()</code> The packet      then enters the OUTPUT chain, at the end of which <code>dst_output()</code> is called.      On finding the packet's route, <code>skb_dst(dst)-&gt;output</code> is set to <code>ip_output</code>.</p>
<p>The skb then enters the POSTROUTING chain, at the end of which <code>ip_finish_output()</code>      is called. <code>ip_finish_output()</code> checks if the packet needs fragmentation (in  certain cases, the packet might have modified or the packet route might change, which may require ip fragmentation). Ignoring the fragmentation, <code>ip_finish_output2()</code> is called.</p>
<pre><code>		            transport layer (TCP/UDP)

					                        🠗       __ip_local_out()
                                          OUTPUT
		      INPUT                         🠗       dst_output()
		        |                           |
		ROUTING DECISION --- FORWARDING --- +
		        |                           |
		    PREROUTING                      🠗       ip_output()
		        |                      POSTROUTING
		                                    🠗       ip_finish_output() 

		             CORE NETWORKING 
</code></pre>
<p>​      <code>ip_finish_output2()</code> first checks if the interface the packet is being  routed out to has a corresponding neighbour entry (<code>neigh</code>). The neighbour subsystem is how the kernel manages link local connections corresponding to IP addresses. i.e. ARP to manage ipv4 addresses and NDISC for ipv6 addresses. If the next hop for an interface is not known, the corresponding messages      are triggered, and is added to the corresponding cache. The current arp      cache can be checked by printing <code>/proc/net/arp</code>. For now, the assuming      the neigh can be found, we will proceed. <code>neigh_output</code> is called, which      calls <code>neigh_hh_output()</code>. (An output function is registered in the neigh entry, which will be called if the neigh entry has expired. Ignoring this for now.)</p>
<p>​     <code>neigh_hh_output()</code> adds the hardware header necessary into the headroom. The neigh entry contains a cached hardware header, which is added while      adding a neigh entry into the neigh cache (after a successful ARP resolution      or neighbour discovery) is complete. More of this will be covered in a      separate page covering the neigh subsystem.</p>
<p>Now the skb has all necessary headers, pass it to the core networking subsystem which will let the driver send the packet out.</p>
<pre><code>static int ip_finish_output2()
{
    neigh = __ipv4_neigh_lookup_noref(dev, nexthop);
    neigh_output(neigh, skb)
    {
        hh_alen = HH_DATA_ALIGN(hh_len); //align hard header
        memcpy(skb-&gt;data - hh_alen, hh-&gt;hh_data,
                hh_alen);
    }
    __skb_push(skb, hh_len); // add the hh header
    return dev_queue_xmit(skb);
} 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n35-38-net_tx-and-driver-xmit"><a class="header" href="#n35-38-net_tx-and-driver-xmit">N3.5-3.8 NET_TX and driver xmit</a></h1>
<h3 id="n35-core-networking"><a class="header" href="#n35-core-networking">N3.5 Core networking</a></h3>
<p>​      The current section will cover the simplest case of sending out a packet.      NET_TX softirq will not be scheduled, in most cases packets will be sent      out this way.</p>
<p>​      Every real network device on creation has atleast one TX queue (var &quot;txq&quot;).      By real I mean actual physical devices: ethernet or wifi interfaces.      Virtual network devices like loopback, tun interfaces, etc are &quot;queueless&quot;,      i.e. they have a txq and a default Queue Discipline (<code>qdisc</code>), but a function is not added to      enqueue packets.</p>
<p>Run &quot;tc qdisc show dev lo&quot;, and it should show &quot;noqueue&quot; as the only queue. Other real devices have queues with  specific properties, as shown below eth0 has a <code>fq_codel</code> queue. I'll add a separate page for qdiscs, for now ignore them.</p>
<pre><code class="language-bash">$ tc qdisc show dev lo
qdisc noqueue 0: root refcnt 2

$ tc qdisc show dev eth0
qdisc fq_codel 0: root refcnt 2 limit 10240p flows 1024 quantum 1514 target 5.0ms
interval 100.0ms memory_limit 32Mb ecn 
</code></pre>
<p>A small optional section on how loopback's xmit works is added next. For now assume our device has a queue.</p>
<p><code>__dev_queue_xmit()</code> finds the tx queue and qdisc and if a enqueue function      is present, calls <code>__dev_xmit_skb()</code> which calls the function to enqueue the      skb into the <code>qdisc</code>. At this point the skb is in the queue. We move  forward without any skb pointer held. After enqueueing the skb,      <code>__qdisc_run()</code> is called to process packets (if possible) that have been      enqueued. If no other process needs the cpu and less than 64 packets      have been processed in the current context, <code>__qdisc_run()</code> will continue      processing packets. <code>__qdisc_run</code> calls <code>qdisc_restart()</code> internally, which      dequeues skbs from the queue and calls <code>sch_direct_xmit()</code>, which calls      <code>dev_hard_start_xmit()</code> and it finally calls <code>xmit_one()</code>. <code>xmit_one()</code> will transmit one skb from the queue. </p>
<pre><code class="language-c">__dev_queue_xmit()
{
    struct netdev_queue *txq;
    struct Qdisc *q;

    txq = netdev_pick_tx(dev, skb, sb_dev);
    q = rcu_dereference_bh(txq-&gt;qdisc);
    rc = __dev_xmit_skb(skb, q, dev, txq)
    {
        rc = q-&gt;enqueue(skb, q, &amp;to_free) &amp; NET_XMIT_MASK;
        __qdisc_run(q)
        {
            //while constraints allow
            qdisc_restart(q, &amp;packets)
            {
                skb = dequeue_skb(q);
                sch_direct_xmit(skb);
            }
        }
        qdisc_run_end(q);
    }
} 
</code></pre>
<h3 id="n36-net_tx-optional"><a class="header" href="#n36-net_tx-optional">N3.6 NET_TX (OPTIONAL)</a></h3>
<p>Ironic that the article on NET_TX has this section marked as OPTIONAL. But      in simple scenarios, NET_TX softirq is almost never raised. After enqueueing      the packet,  <code>__qdisc_run()</code> cannot process packets because if one of these conditions is not met:</p>
<ol>
<li>no other process is waiting for this CPU</li>
<li>the current process has enqueued less than 64 packets.</li>
</ol>
<p>Then a NET_TX is scheduled to run. <code>__netif_schedule()</code> will raise a      NET_TX softirq if it was not already triggered on the CPU. <code>net_tx_action()</code>, the      registered function will run <code>__qdisc_run()</code> on the qdisc, after which the      flow is same as the case without a softirq raised.</p>
<p>Though we have not covered NAPI yet, <code>net_tx_action()</code> is a dual of <code>net_rx_action()</code>,  a root <code>qdisc</code> is a dual of a <code>napi</code> structure, and <code>xmit_one()</code> is a dual of <code>__netif_receive_skb()</code>, with very similar logic but in opposite directions.</p>
<h3 id="n37-driver-xmit_one"><a class="header" href="#n37-driver-xmit_one">N3.7 driver xmit_one()</a></h3>
<p>​      <code>xmit_one()</code> is the final function, which like <code>__netif_receive_skb()</code> shares      the packet with all registered promiscuous packet_types, i.e. the      global list <code>ptype_all</code> and per netdevice list <code>dev-&gt;ptype_all</code>. This is done      within <code>dev_queue_xmit_nit()</code> function. After this, <code>xmit_one</code> calls      <code>netdev_start_xmit()</code> which internally calls <code>__netdev_start_xmit()</code> to      hand over the packet to the driver by calling <code>ops-&gt;ndo_start_xmit()</code>      (ndo stands for NetDevice Ops). A <code>net_device_ops</code> struct is registered      during netdevice creation, where this function pointer is set by the driver. The driver will then send the packet out via the physical interface.</p>
<pre><code class="language-c">sch_direct_xmit() -&gt; dev_hard_start_xmit() -&gt; xmit_one()
{
    dev_queue_xmit_nit();
    //deliver skb to promisc packet types

    netdev_start_xmit()
    {
        const struct net_device_ops *ops = dev-&gt;netdev_ops;
        return ops-&gt;ndo_start_xmit(skb, dev);
    }
} 
</code></pre>
<h3 id="n38-loopback-xmit-optional"><a class="header" href="#n38-loopback-xmit-optional">N3.8 loopback xmit (OPTIONAL)</a></h3>
<p>​      Like described earlier, each device during creation registers certain      functions using the <code>net_device_ops</code> structure. Loopback registers      <code>loopback_xmit</code> as the function. On sending a packet to loopback, when      <code>ops-&gt;ndo_start_xmit</code> is called, the packet enters <code>loopback_xmit()</code>. It is      a very simple function, which increments stats and calls <code>netif_rx_ni()</code>      to begin the RX path of the packet. The end of TX coincides with the start of RX in this function. </p>
<p>​      The loopback device is described in <code>drivers/net/loopback.c</code> .</p>
<pre><code class="language-c">loopback_xmit()
{
    skb_tx_timestamp(skb);
    skb_orphan(skb);    // remove all links to the skb.
    skb-&gt;protocol = eth_type_trans(skb, dev);    // set protocol

    netif_rx(skb);    //RX!!
} 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="n3-wip-socket-programming-bts"><a class="header" href="#n3-wip-socket-programming-bts">N3. (WIP) Socket Programming BTS</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="m-miscellaneous"><a class="header" href="#m-miscellaneous">M. Miscellaneous</a></h1>
<p>Either notes that I have not sorted or ones that don't require a dedicated section.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cse-222a-notes"><a class="header" href="#cse-222a-notes">CSE 222a, Notes</a></h1>
<p>This page contains summaries of the various papers I read as a part of CSE222a, at UCSD. It was taught by Prof. George Porter. The course focused on reading classical and current papers followed by in class discussions. The following are papers that I found interesting. Most of the summaries usually consist of four paragraphs:</p>
<ol>
<li>the introduction, problem its trying to solve, the pain points</li>
<li>the idea, how it solves the problem</li>
<li>their implementation</li>
<li>their results and my thoughts</li>
</ol>
<p>Some of these papers cannot be uploaded/shared. I have provided links to some of the papers if I was able to find a public link off DuckDuckGo. Else I have provided a private link which needs subscription. In case you cannot access them, send me a mail and if possible I'll share the paper.</p>
<h3 id="m11-early-internet-darpa-internet-protocols"><a class="header" href="#m11-early-internet-darpa-internet-protocols">M1.1 Early Internet, Darpa Internet Protocols</a></h3>
<p>These papers describe the structure of an early internet. The first paper is a description of the components in building a packet switching network. The second one, published 14 years later is a description the design goals and the protocols developed.</p>
<h5 id="paper-1-a-protocol-for-packet-network-incommunication-ieee-1974-a-hrefhttpswwwcsprincetoneducoursesarchivefall06cos561paperscerf74pdflinka"><a class="header" href="#paper-1-a-protocol-for-packet-network-incommunication-ieee-1974-a-hrefhttpswwwcsprincetoneducoursesarchivefall06cos561paperscerf74pdflinka">Paper 1. A Protocol for Packet Network Incommunication (IEEE 1974) [<a href="https://www.cs.princeton.edu/courses/archive/fall06/cos561/papers/cerf74.pdf">Link</a>]</a></h5>
<p>The paper first clearly defines the components of the network viz Hosts, networks and gateways. The hosts as the end points which are connected to each other. Multiple hosts are connected to internetwork. And internetworks are connected via Gateways. Each internetwork can use its own protocol for communication. But communication across internetworks needed a common protocol. The protocol for internetwork communication is the early IP protocol. The authors then describe a protocol over the internetwork protocol to transfer data between end points.</p>
<p>The protocols described are similar to modern implementations of IP and TCP headers. The original vision of the packet switching network remains very similar to our current networks. The paper clearly explains their choice for a common internetwork protocol, while each network was free to develop protocols for communication within the network. Th Gateway simply handled translation between the network protocols. This is very clearly the design that we still adopt, except that hosts very rarely communicate on bare link local protocol. Most communication, even between hosts within a network, is over IP. The authors choice to have a common IP protocol implemented by all hosts which we have acknowledge today as the narrow waist of the internet. Other differences include fragmentation using sequence numbers, which have now moved to TCP, while IP implements a blunt form of fragmentation.</p>
<p><img src="M/imgs/M1_interntwhdr.svg" alt="M1_interntwhdr" /></p>
<p>The authors describe the transmission control protocol as a means to transmit a stream of data, which it breaks into segments. Early TCP used the sequence numbers provided by internetwork protocol, and both the protocols worked closely to transmit data. Modern IP and TCP have been separated into distinct protocols with independent functions. The authors also have a section which describes early address format and address assignment which have completely changed. The authors also envision a central repository that provides addresses for each node within a network, which in way the current DHCP implementation.</p>
<p>The paper is a description of the network that the authors envisioned, some of their decisions have formed the basis of the current internet.</p>
<h5 id="paper-2-the-design-philosophy-of-the-darpa-internet-protocols-acm-1988-a-hrefhttpsgroupscsailmiteduanapublicationspubpdfsthe20design20philosophy20of20the20darpa20internet20protocolspdflinka"><a class="header" href="#paper-2-the-design-philosophy-of-the-darpa-internet-protocols-acm-1988-a-hrefhttpsgroupscsailmiteduanapublicationspubpdfsthe20design20philosophy20of20the20darpa20internet20protocolspdflinka">Paper 2. The Design Philosophy of the DARPA Internet Protocols (ACM 1988) [<a href="https://groups.csail.mit.edu/ana/Publications/PubPDFs/The%20design%20philosophy%20of%20the%20DARPA%20internet%20protocols.pdf">Link</a>]</a></h5>
<p>The second paper describes the philosophy behind the design of the Internet protocols after the protocols had standardised. The paper first describes the fundamental goals of the network. Then they describe how these goals influence the design.</p>
<p>One of the foremost goals was for the network to and continue working despite loss. This goal was adopted since the network was initially a military project that was later released for civil use. This made the internet fault tolerant at the expense of performance. To achieve this, the state necessary for internet functioning is moved to the endpoints. This approach is described as &quot;fate sharing&quot; since both entity and the state share the same fate. A consequence of this approach was the stateless design of switches and gateways. Also, each end node is responsible for maintaining any state necessary for the connection. It is interesting that the goal and the chosen approach is applied in current network designs.</p>
<p>The paper then describes the services supported over IP and the need for a protocol that allows the user to use the datagram services offered by IP. This created a need for decoupling IP and TCP, and building the User Datagram Protocol as the protocol which offers these services. The paper then describes the division into layers which offer services independently using services offered by lower layers. The classical OSI model is the modern form of this idea, with clear abstractions of the functions provided by each layer.</p>
<h3 id="m12-distributed-computation-of-a-spanning-tree-in-an-extended-lan"><a class="header" href="#m12-distributed-computation-of-a-spanning-tree-in-an-extended-lan">M1.2 Distributed Computation of a Spanning Tree in an Extended LAN</a></h3>
<h5 id="paper-an-algorithm-for-distributed-computation-of-a-spanning-tree-in-an-extended-lan-acm-1985-a-hrefhttpswwwituuseeducoursehomepagedatakomht06slidessta-perlmanpdflinka"><a class="header" href="#paper-an-algorithm-for-distributed-computation-of-a-spanning-tree-in-an-extended-lan-acm-1985-a-hrefhttpswwwituuseeducoursehomepagedatakomht06slidessta-perlmanpdflinka">Paper: An Algorithm for Distributed Computation of a Spanning Tree in an Extended LAN (ACM 1985) [<a href="https://www.it.uu.se/edu/course/homepage/datakom/ht06/slides/sta-perlman.pdf">Link</a>]</a></h5>
<p>The author describes a algorithm to build a tree from an extended LAN. As networks began to grow, with LANs interconnected using bridges to build an extended LAN, manual configuration of the bridges increased the risk of loops. Similarly a user without proper knowledge could connect LANs with a bridge and create a Loop. The proposed algorithm allows the bridges to exchange data and construct a tree without any additional configuration. Secondly the algorithm converges in O(mesh diameter) while imposing small memory requirements (proportional to the number of LANs the bridge is connected to) on each bridge.</p>
<p><img src="M/imgs/M1_extended_LAN.svg" alt="extended_lan" /></p>
<p>In the final tree, each LAN has a designated bridge. On receiving data from a LAN, a bridge forwards the data to all other LANs on which it is a designated bridge. The complete mesh is transformed into a tree by choosing the designated bridges. The root of the tree is a designated bridge on all of it's LANs. The algorithm chooses a very simple way to choose the root. The bridge with the smallest MAC value is chosen as the root. The bridges exchange HELLO messages to find the root. Once a root is found, the root sends out periodic HELLO messages. These HELLO messages are forwarded on all connected LANs. The bridges based on the HELLO messages share information and choose the designated bridge on each LAN. The algorithm works by each bridge first trying to become the root and then trying to become the designated brigde on each of the LANs it is connected to.</p>
<p>The paper also describes states maintained by the bridge for each LAN and transitions between states to prevent loops due to new bridges being added. Also, the HELLO messages timeout letting the bridge detect failures in the network. This has been described in detail in the paper and I cannot sufficiently describe it in a single paragraph.</p>
<p><img src="M/imgs/M1_ex_LAN_tree2.svg" alt="ext_bridge" /></p>
<p>Bridge 1 becomes the leader, dotted links are in backup state. Arrows are links in forwarding state, pointing towards non leader bridges.</p>
<p>It is always nice to see a simple distributed algorithm which solves the problem. What is especially refreshing is the poem she wrote as a part of the abstract, which summarises the algorithm succinctly.</p>
<p><em>I think that I shall never see</em>
<em>A graph more lovely than a tree</em></p>
<p><em>A tree whose crucial property</em>
<em>Is loop-free connectivity.</em></p>
<p><em>A tree which must be sure to span</em>
<em>So packets can reach every LAN.</em></p>
<p><em>First the Root must be selected</em>
<em>By ID it is elected.</em></p>
<p><em>Least cost paths from Root are traced.</em>
<em>In the tree these paths are placed</em></p>
<p><em>A mesh is made by folks like me</em>
<em>Then bridges find a spanning tree.</em></p>
<h3 id="m13-network-protocol-folklore"><a class="header" href="#m13-network-protocol-folklore">M1.3 Network Protocol Folklore</a></h3>
<h5 id="paper-network-protocol-folklore-acm-2019-a-hrefhttpsccronlinesigcommorgwp-contentuploads201910acmdl19-336pdflinka"><a class="header" href="#paper-network-protocol-folklore-acm-2019-a-hrefhttpsccronlinesigcommorgwp-contentuploads201910acmdl19-336pdflinka">Paper: Network Protocol Folklore (ACM 2019) [<a href="https://ccronline.sigcomm.org/wp-content/uploads/2019/10/acmdl19-336.pdf">Link</a>]</a></h5>
<p>Radia Perlman, having been working on internet protocols since it's inception, shares a history of choices they made and their consequences on the modern internet implementation. One line from the abstract which makes the paper interesting is: Some decisions have made today’s networks unnecessarily complex and less functional. Surprisingly, mechanisms that were created out of necessity, to compensate for previous decisions, sometimes turn out to be useful for purposes other than the original reason they were invented.</p>
<p>Ethernet addresses were designed to be 6 bytes long so that nodes connected over ethernet would not need any other address configuration. But ethernets never have more than a few thousand nodes attached. Each ethernet address is unique and location independent, but are simply used for link to link transfer. It is interesting that IPv4, which was designed so each node would get a unique address dynamically has a smaller length of just 4 Bytes. People even envisioned ethernet as Layer 3 replacement, implementing end to end connections over ethernet due to the uniqueness property of the ethernet address.</p>
<p>DEC had developed an alternate to IP called IS-IS which adopted CLNP (ConnectionLess Network Protocol) addressing. CNLP has very large 20 Byte addresses ( larger than 16Byte IPv6). Though it did not catch on CNLP supported features that were later built into IPv6.</p>
<p>The paper has a brief overview of the history behind the spanning tree protocol. Customers expected nodes in different ethernets to work. DEC sold routers which routed based on Layer 3 addresses, but expected end nodes to implement Layer 3 to route between different ethernets. The author argues that the right solution was for all endpoints to put Layer 3 into the stack for such inter ethernet communication. But, DEC decided that instead of waiting for endpoints to upgrade, it was easier to build a network box which routed based on ethernet addresses, i.e. the bridge. Radia Perlman was asked to design an algorithm to break ethernet routing loops. The resulting algorithm, the spanning tree algorithm, a temporary fix till endpoints upgrade their network stacks is still being widely deployed.</p>
<p>The paper compares CLNP and modern IPv6. A few features of CLNP are:</p>
<ul>
<li>Autoconfiguration by appending 14 byte network prefix with the 6 Byte ethernet address to build a complete 20Byte address.</li>
<li>The network prefix is hierchically assigned based on region.</li>
<li>Within an area since the prefix is common, nodes can move anywhere in the area without changing the address.</li>
<li>Since CLNP had ethernet address, ethernet header could be dropped so CLNP is used for both Layer 2 and 3 routing.</li>
</ul>
<p>IPv4 addressing was not as scalable as CLNP and does not support movement within an area without address reconfiguration (especially helpful with Datacenters migrating VMs). IP is a flat address space with ethernet providing functionality of a region. One consequence of IP and ethernet being completely independent is that we need the ARP protocol to discover the endnode's ethernet address. CLNP on the other hand needed each node to anounce it's location.</p>
<p>The paper then moves to the final section which are surprising outcomes of the decisions people took.</p>
<ul>
<li>Seperation of IP and Ethernet allowed VLAN, which provides the ability to configure which nodes can talk. This is used in datacenters to build a LAN for a group of VMs.</li>
<li>Since CLNP uses a unique ethernet address which is then announced to everyone, tracking using CLNP would have been easier making it a major privacy issue. Instead since IP is ephemeral, tracking though possible is less intrusive.</li>
<li>NATs were built to fix IPv4's inability to scale, not necessary for CLNP. But NATs have become a major security feature to obfusicate users behind a NAT. Also since users behind a NAT cannot be contacted without the user initiating a connection, the users are protected from external nodes.</li>
</ul>
<hr />
<h3 id="the-ones-below-are-incomplete"><a class="header" href="#the-ones-below-are-incomplete">The ones below are incomplete</a></h3>
<h3 id="m14-classic-multicast-routing"><a class="header" href="#m14-classic-multicast-routing">M1.4 Classic Multicast Routing</a></h3>
<p>Multicast Routing in Datagram Internetworks and Extended LANs</p>
<p>The paper is the seminal paper on how multicast can be implemented with support from the bridges. The paper proposes minor modifcations to routing algorithms to facilitate multicast.</p>
<h3 id="m15-bgp-interdomain-routing"><a class="header" href="#m15-bgp-interdomain-routing">M1.5 BGP: Interdomain Routing</a></h3>
<p>eBGP and iBGP, sharing routing information.</p>
<h3 id="m16-end-to-end-routing-behavior"><a class="header" href="#m16-end-to-end-routing-behavior">M1.6 End to End Routing Behavior</a></h3>
<p>Radia Perlman</p>
<h3 id="m17-narada-end-system-multicast"><a class="header" href="#m17-narada-end-system-multicast">M1.7 Narada: End System Multicast</a></h3>
<p>Radia Perlman</p>
<h3 id="m18-active-network-architecture"><a class="header" href="#m18-active-network-architecture">M1.8 Active Network Architecture</a></h3>
<p>Radia Perlman</p>
<h3 id="m19-click-modular-router"><a class="header" href="#m19-click-modular-router">M1.9 Click Modular Router</a></h3>
<p>Radia Perlman</p>
<h3 id="m110-open-flow"><a class="header" href="#m110-open-flow">M1.10 Open Flow</a></h3>
<p>Radia Perlman</p>
<h3 id="m111-congestion-avoidance-and-control"><a class="header" href="#m111-congestion-avoidance-and-control">M1.11 Congestion Avoidance and Control</a></h3>
<p>Reno TCP</p>
<h3 id="m112-fast-switched-backplane-for-a-gigabit-switched-router"><a class="header" href="#m112-fast-switched-backplane-for-a-gigabit-switched-router">M1.12 Fast Switched Backplane for a Gigabit Switched Router</a></h3>
<p>iSlip</p>
<h3 id="m113-p4-programming-protocol-independent-packet-processors"><a class="header" href="#m113-p4-programming-protocol-independent-packet-processors">M1.13 P4: Programming Protocol Independent Packet Processors</a></h3>
<p>p4
M1.14 SWAN</p>
<h3 id="swan-achieving-high-utilization-with-software-driven-wan"><a class="header" href="#swan-achieving-high-utilization-with-software-driven-wan">SWAN: Achieving High Utilization with Software Driven WAN</a></h3>
<p>M1.15 Fat Trees</p>
<h3 id="scalable-commodity-data-center-network-arch"><a class="header" href="#scalable-commodity-data-center-network-arch">scalable commodity data center network arch</a></h3>
<h3 id="m116-portland"><a class="header" href="#m116-portland">M1.16 PortLand</a></h3>
<p>fault tolerant l2 data center</p>
<h3 id="m117-jellyfish"><a class="header" href="#m117-jellyfish">M1.17 Jellyfish</a></h3>
<h3 id="m118-dctcp"><a class="header" href="#m118-dctcp">M1.18 DCTCP</a></h3>
<h3 id="m119-mptcp"><a class="header" href="#m119-mptcp">M1.19 MPTCP</a></h3>
<h3 id="m120-open-vswitch"><a class="header" href="#m120-open-vswitch">M1.20 Open vSwitch</a></h3>
<h3 id="m121-eyeq"><a class="header" href="#m121-eyeq">M1.21 EyeQ</a></h3>
<h3 id="m122-censorship-in-china"><a class="header" href="#m122-censorship-in-china">M1.22 Censorship in China</a></h3>
<p>great firewall</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cache-side-channel-attacks"><a class="header" href="#cache-side-channel-attacks">Cache Side Channel Attacks</a></h1>
<p>By design caches are transparent, but processes can create contention resulting in timing variations.    CPU data cache timing attacks are to exploit cache timing to expose cache's internal state. </p>
<p>Caches are one place where time variations can be exposed.  Caches are built to bridge the gap between main memory and Processor  registers. Since it takes fewer cycles to read from cache than from main memory, recording time differences makes reading from cache detectable.</p>
<h3 id="caches-recap"><a class="header" href="#caches-recap">Caches Recap:</a></h3>
<ul>
<li>Caches are designed to exploit spatial locality in memory access</li>
<li>For each read, check if the data is present in one of the  caches. If present, read from the cache or load from lower memory. LRU  to evict old entries. </li>
<li>Cold cache misses are when the cache is empty, happens during program load.</li>
<li>Data can only be added to certain regions in the cache.  Multiple address lines in lower memory map to a subset in the cache.  Conflict miss happens when data that map to the same region are being  loaded, causing unnecessary cache evictions.</li>
<li>If cache size is larger than the working set of memory, we face capacity misses.</li>
<li>Write through cache: each write is immediately written to lower levels. Simple logic, but increases bus traffic</li>
<li>Write-back cache: write to cache, defer updating lower levels till it is evicted from the cache.</li>
<li>Cache Coherence: The same data can be present on multiple  caches, if one of the processors makes a change to common data, the  processor needs to make sure that cached data is consistent.</li>
<li>A Cache Coherence protocol makes sure of cache coherence  among caches. Cache coherence protocols described below are snooping  protocols, i.e. changes to cache traffic is visible to all cores.</li>
<li>Write-invalidate: On detecting a write to another cached location, invalidate local copy, forcing a read from main memory.</li>
<li>Write-update: On detecting a write, update local copy.</li>
<li>Any cache coherence protocol increases bus traffic. </li>
<li>Article on cache coherence : <a href="https://fgiesen.wordpress.com/2014/07/07/cache-coherency/">https://fgiesen.wordpress.com/2014/07/07/cache-coherency/</a> </li>
<li>Notes on CPU caches: <a href="https://www.cse.iitb.ac.in/%7Emythili/os/notes/notes-cache.txt">https://www.cse.iitb.ac.in/~mythili/os/notes/notes-cache.txt</a>
<ul>
<li>has a clean explanation of MESI cache coherence protocol.</li>
</ul>
</li>
<li>Code patterns to improve cache performance:
<ul>
<li>Build loops, which operate on the same data. Merge loops if necessary</li>
<li>Within a data structure, make sure critical (read/write heavy) elements are cache aligned and at the top.</li>
<li>Reduce branches so code can be prefetched.</li>
<li>Atomic variables are allowed to be present only in one  cache. Use of atomic variables (and hence locks and synchronization  mechanisms) on multiple cores increases cache coherence traffic. </li>
<li>Instead try to use per CPU data structures.</li>
</ul>
</li>
</ul>
<h3 id="flushreload-paradigm"><a class="header" href="#flushreload-paradigm">Flush+Reload Paradigm.</a></h3>
<ul>
<li>Exploits Cache behavior to leak information about victim process.</li>
<li>Flush a line, wait for some time, measure access time. If  the victim has accessed the memory, access time will be in cache and  access will be fast. Else will be slow.</li>
<li>Repeat to identify victim's memory regions.</li>
<li>Slides Link: <a href="https://cs.adelaide.edu.au/%7Eyval/CHES16/">https://cs.adelaide.edu.au/~yval/CHES16/</a></li>
</ul>
<h3 id="spectre-1-bounds-check-bypass"><a class="header" href="#spectre-1-bounds-check-bypass">Spectre 1: Bounds Check Bypass:</a></h3>
<ul>
<li>
<p>Modern CPUs can speculatively execute instructions in a branch, before a branch true execution starts. </p>
</li>
<li>
<pre><code>int some_function(int user_input)
{
    int x = 0;
    int flush_reload_buffer[256];
    if (check user_input) {  // on checking user_input, undo changes to x
        x = secret_array[user_input];  // this is executed. before check.
                                       // contents leak into the cache.
    }
    y = flush_reload_buffer[x];
    leak = measure_access_time_of_all_elements(flush_reload_buffer);
} 
</code></pre>
</li>
<li>
<p>The CPU based on historic data, tries to predict branches and  executes instructions in the branch. Later it checks the branch check,  if the instructions it ran were incorrect, it undoes all the changes.  These changes are transparent for the user. But the instructions in the  branch load data into the cache. This data can then be identified by  cache timing.</p>
</li>
<li>
<p>This can be used to read data from the kernel via eBPF JIT.  Users can provide eBPF  bytecode which the kernel runs at certain kernel hooks. The bytecode is translated into machine code using a JIT engine.</p>
</li>
<li>
<p>HW Solution is to clean up cache while undoing changes. </p>
</li>
</ul>
<h3 id="spectre-2-branch-target-injection"><a class="header" href="#spectre-2-branch-target-injection">Spectre 2: Branch target injection:</a></h3>
<ul>
<li>Independent execution of the same code, can influence branch predictions. Earlier people worked on inferring  details about the  victim based on attacker's branch prediction. This attack instead forces victim's branch prediction due to attacker's choice of branch  execution.</li>
<li>Most branch predictors maintain a table of branches to jump  addresses. The last 12 bits are used to identify the branches and  addresses.</li>
<li>Since Only the last 12 bits are used, the attacker fills the table with branch and jump entries. The CPU while executing the victim  code enters the branch and loads the secret into the cache, which is  then read by the attacker. Note that the victim might itself have never  entered the branch, but the CPU speculatively executes the branch.</li>
<li>This can be used to read the host's memory from a KVM guest.</li>
<li>HW Solution: Update microcode so OS can control branch prediction table. </li>
<li>SW Retpoline Solution: Retpoline = return + trampoline. An  indirect branch that is not subject to speculative execution. Instead of a &quot;jmp&quot; use a call and return. <a href="https://support.google.com/faqs/answer/7625886">Google Blog.</a></li>
</ul>
<h3 id="spectre-3-rogue-data-cache-load"><a class="header" href="#spectre-3-rogue-data-cache-load">Spectre 3: Rogue data cache load</a></h3>
<ul>
<li>
<p>This attack exploits the fact that the CPU executes instructions in parallel, sometimes out of order to achieve parallelism.</p>
</li>
<li>
<pre><code>mov rax, [Somekerneladdress]
mov rbx, [someusermodeaddress] 
</code></pre>
</li>
<li>
<p>In the above example, the first instruction causes an  exception. But if the CPU runs both of them in parallel, the second one  can load user data into the cache before the exception occurs. Using  Flush+Reload data address can be identified.</p>
</li>
</ul>
<h3 id="meltdown"><a class="header" href="#meltdown">Meltdown:</a></h3>
<ul>
<li>This attack specifically tries to read kernel data from  userspace. It does this by trying to load a kernel address. The CPU loads the address and then raises an exception. This loads the data into the  cache which is then read using Flush+Reload. Similar to Spectre v3 but  the implementation is specific to Intel.</li>
</ul>
<h3 id="ridl"><a class="header" href="#ridl">RIDL:</a></h3>
<ul>
<li>Other parts of the CPU from which similar data can leak. <a href="https://mdsattacks.com/">https://mdsattacks.com/</a></li>
<li><a href="https://mdsattacks.com/slides/slides.html">https://mdsattacks.com/slides/slides.html</a></li>
</ul>
<h3 id="mitigations"><a class="header" href="#mitigations">Mitigations</a></h3>
<p>KAISER: an implementation to hide kernel data from userspace. <a href="https://lwn.net/Articles/738975/">Link</a></p>
<p>Meltdown and Spectre cause a performance Impact of 8 to 20% to IO intensive workloads, &lt;5 % impact on CPU intensive. (<a href="https://access.redhat.com/articles/3307751">Source</a>)</p>
<p>On my AMD Ryzen 5 PRO 3500U Laptop:</p>
<pre><code>$ grep . /sys/devices/system/cpu/vulnerabilities/*
itlb_multihit:Not affected
l1tf:Not affected
mds:Not affected
meltdown:Not affected
spec_store_bypass:Mitigation: Speculative Store Bypass disabled via prctl and seccomp
spectre_v1:Mitigation: usercopy/swapgs barriers and __user pointer sanitization
spectre_v2:Mitigation: Full AMD retpoline, IBPB: conditional, STIBP: disabled, RSB filling
tsx_async_abort:Not affected 
</code></pre>
<ul>
<li>Google zero blog <a href="https://googleprojectzero.blogspot.com/2018/01/reading-privileged-memory-with-side.html">link</a> .</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="a-appendix"><a class="header" href="#a-appendix">A. Appendix</a></h1>
<p>These are a few links I think are useful.</p>
<ol>
<li>David Miller's <a href="http://vger.kernel.org/%7Edavem/skb.html">How SKBs work</a> and <a href="http://vger.kernel.org/%7Edavem/skb_data.html">skb data</a>. His home page has <a href="https://duckduckgo.com/?q=site%3A%22http%3A%2F%2Fvger.kernel.org%2F%7Edavem%2F%22+html">other useful links</a> as well.</li>
<li><a href="https://lwn.net">lwn.net</a> contains clean summaries. Search for links from site, e.g. lwn pages about <a href="https://duckduckgo.com/?q=site%3A%22lwn.net%22+ebpf">eBPF</a>.
<ol start="3">
<li>LWN page with Kernel related articles grouped by topic: <a href="https://lwn.net/Kernel/Index/">Kernel index</a>.</li>
</ol>
</li>
<li><a href="lartc.org">lartc.org</a> the best place for routing, traffic shaping, tunnelling info.</li>
<li>Twitter has a lot of active netdev members. <a href="https://twitter.com/tejaswi_tan/lists/netdev1">my list</a></li>
</ol>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
                
    </body>
</html>
